{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(\"../../..\")\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso \n",
    "from sklearn.decomposition import PCA #USE PCA FOR PCR REGRESSION\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import math\n",
    "from tslib.src import tsUtils\n",
    "from tslib.src.synthcontrol.syntheticControl import RobustSyntheticControl\n",
    "from tslib.tests import testdata\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import warnings\n",
    "from scipy.linalg import LinAlgWarning\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "import pyro.infer.mcmc as mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a get_training_data function that extracts data from our cleaned data source stadium_county_df \n",
    "that we obtained in our reproduction file Synth_Control_Paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stadium data for obtaining training data \n",
    "stadium_county_df = pd.read_csv('Cleaned_stadium_data.csv')\n",
    "\n",
    "\n",
    "#Grouped data for running Bayesian SC\n",
    "grouped_df = pd.read_csv('Grouped_df.csv')\n",
    "\n",
    "county_covid = pd.read_csv('County_Covid_Data.csv')\n",
    "\n",
    "county_covid['date'] = pd.to_datetime(county_covid['date'])\n",
    "county_covid = county_covid.loc[~(county_covid['county'] == 'Unknown')]\n",
    "county_covid = county_covid.loc[~(county_covid['cases'].isnull())]\n",
    "county_covid['county'] = county_covid['county'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Ignore LinAlgWarning\n",
    "warnings.filterwarnings(\"ignore\", category=LinAlgWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_state_dict = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below function gets training X and training Y data for applying Bayesian Model \n",
    "def get_training_data(team_name_str, stadium_county_str, state_str, intervention_date, show_plot, week):\n",
    "\n",
    "    stadium_county_str = [x for x in stadium_county_str if x != \"\"]\n",
    "    state_str = [x for x in state_str if x != \"\"]\n",
    "    intervention_date = [x for x in intervention_date if x != \"\"]\n",
    "    #Convert to lower case to avoid case insensitivity later\n",
    "    stadium_county_str = [s.lower() for s in stadium_county_str]\n",
    "    #To find intervention_date, we want first entry that has numbers, since some in data is text only.\n",
    "\n",
    "    for s in intervention_date:\n",
    "        if any(c.isdigit() for c in s):\n",
    "            intervention_date = s\n",
    "\n",
    "\n",
    "    intervention_date = pd.to_datetime(intervention_date)\n",
    "\n",
    "\n",
    "    #Convert State Acronym to full state name\n",
    "    for state in range(len(state_str)):\n",
    "        if state_str[state] in home_state_dict:\n",
    "            state_str[state] = home_state_dict[state_str[state]]\n",
    "            \n",
    "            \n",
    "    #print(stadium_county_df)\n",
    "    #Find Synthetic Counties\n",
    "    synthetic_counties = list(stadium_county_df.loc[stadium_county_df['Team'] == team_name_str]['Donor_Counties'])[0]#.copy()\n",
    "    #synthetic_counties = list(synthetic_counties)\n",
    "    synthetic_counties = eval(synthetic_counties)\n",
    "    synthetic_counties = [s.lower() for s in synthetic_counties]\n",
    "    n_donors = len(synthetic_counties)\n",
    "    \n",
    "    #print(synthetic_counties)\n",
    "\n",
    "    #Find Dataframe of X and Y data\n",
    "    #Special case where \n",
    "    \n",
    "    \n",
    "    if team_name_str == 'Washington':\n",
    "        stadium_county_data = county_covid.loc[(county_covid['county'].isin(stadium_county_str)) | ((county_covid['county'].isin(synthetic_counties)) & (county_covid['state'] == 'Maryland'))]\n",
    "    else:\n",
    "        stadium_county_data = county_covid.loc[(county_covid['county'].isin(stadium_county_str) | (county_covid['county'].isin(synthetic_counties))) & (county_covid['state'].isin(state_str))]\n",
    "    \n",
    "    stadium_county_data = stadium_county_data.fillna(method='bfill')\n",
    "    stadium_county_data['date'] = pd.to_datetime(stadium_county_data['date'], infer_datetime_format=True)\n",
    "    \n",
    "    earliest_date = list(stadium_county_data.loc[stadium_county_data['county'].isin(stadium_county_str)]['date'])[0]\n",
    "    \n",
    "    #Start training from the earliest date of when our stadium county data becomes available.\n",
    "    stadium_county_data = stadium_county_data.loc[stadium_county_data['date'] >= earliest_date]\n",
    "    \n",
    "    \n",
    "    #Total Pivot is pivot table cases for entire dataset, training pivot is the same but for < intervention date\n",
    "    total_pivot = stadium_county_data.pivot_table(columns='county', values='cases', index= 'date').reset_index()\n",
    "    total_pivot = total_pivot.loc[total_pivot['date'] >= earliest_date]\n",
    "    \n",
    "    #Sum up stadium counties for our prediction. \n",
    "    total_pivot['Stadium_County'] = total_pivot.apply(lambda row: row[stadium_county_str].sum(), axis=1)\n",
    "    #total_pivot['Stadium_County'] = total_pivot.loc[:, total_pivot.columns == (stadium_county_str[0])]\n",
    "    \n",
    "    \n",
    "    total_pivot.drop(stadium_county_str, axis=1, inplace=True)\n",
    "    \n",
    "    total_pivot.fillna(0, inplace=True)\n",
    "    \n",
    "    training_pivot = total_pivot.loc[total_pivot['date'] < intervention_date]\n",
    "    \n",
    "    training_dates = training_pivot['date']\n",
    "    \n",
    "    total_dates = total_pivot['date']\n",
    "    \n",
    "    test_pivot = total_pivot.loc[total_pivot['date'] >= intervention_date]\n",
    "    \n",
    "    test_pivot = test_pivot.drop(['date'], axis=1)\n",
    "    \n",
    "    training_pivot = training_pivot.drop(['date'], axis=1)\n",
    "    \n",
    "    total_pivot = total_pivot.drop(['date'], axis=1)\n",
    "    \n",
    "    \n",
    "    X_train = training_pivot.loc[:, ~training_pivot.columns.isin(['Stadium_County'])]\n",
    "    \n",
    "    Y_train = training_pivot['Stadium_County']\n",
    "    \n",
    "    total_X = total_pivot.loc[:, ~total_pivot.columns.isin(['Stadium_County'])]\n",
    "    total_Y = total_pivot['Stadium_County']\n",
    "    \n",
    "    test_X = test_pivot.loc[:, ~test_pivot.columns.isin(['Stadium_County'])]\n",
    "    test_Y = test_pivot['Stadium_County']\n",
    "    \n",
    "    \n",
    "\n",
    "    return X_train, Y_train, test_X, test_Y, total_dates\n",
    "\n",
    "x_train, y_train, x_test, y_test, dates = get_training_data('Cincinnati',[\"Hamilton\"] ,[\"OH\"], ['10/04/2020'], True, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create the Model structures, all using Pyro to sample. \n",
    "\n",
    "We incorporate a mask input to use for our population predictive checks. \n",
    "\n",
    "The 3 models are Bayesian PCA, Poisson Factorization, and Poisson Gamma Dynamical Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_pca(data, latent_dim, mask): #Pass in mask directly into the model \n",
    "    # Define model parameters\n",
    "    n, p = data.shape\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "    mu = torch.zeros(p)\n",
    "    covariance = sigma * torch.eye(p)\n",
    "    \n",
    "    Z_mean = pyro.param(\"Z_mean\", torch.zeros(n, latent_dim))\n",
    "    Z_cov = pyro.param(\"Z_cov\", torch.eye(latent_dim))\n",
    "\n",
    "\n",
    "    W_mean = pyro.param(\"W_mean\", torch.zeros(latent_dim, p))\n",
    "    W_cov = pyro.param(\"W_cov\", torch.eye(p))\n",
    "    \n",
    "    \n",
    "    Z_mean.data = torch.zeros(n, latent_dim)\n",
    "    Z_cov.data = torch.eye(latent_dim)\n",
    "    W_mean.data = torch.zeros(latent_dim, p)\n",
    "    W_cov.data = torch.eye(p)\n",
    "\n",
    "    Z_nl = pyro.sample(\"Z\", dist.MultivariateNormal(Z_mean, Z_cov))\n",
    "    W_lp = pyro.sample(\"W\", dist.MultivariateNormal(W_mean, W_cov))\n",
    "    \n",
    "    mean = Z_nl @ W_lp\n",
    "\n",
    "    #Implement mask on our data to avoid masked probabilities. \n",
    "\n",
    "    X = pyro.sample(\"X\", dist.Normal(mean, sigma).mask(mask), obs=data)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def poisson_factorization(data, latent_dim, mask):\n",
    "    # Define model parameters\n",
    "    n, p = data.shape\n",
    "    \n",
    "    #Construct samples of F, G, and X. X is assumed to be poisson distribution of F * G\n",
    "    \n",
    "#     alpha = pyro.sample(\"alpha\", dist.Gamma(1., 1.))\n",
    "#     beta = pyro.sample(\"beta\", dist.Gamma(1., 1.))\n",
    "    F = pyro.sample(\"F\", dist.Gamma(1, 1).expand([n, latent_dim]))\n",
    "    G = pyro.sample(\"G\", dist.Gamma(1, 1).expand([latent_dim, p]))\n",
    "    \n",
    "\n",
    "    #use our mask to mask data\n",
    "\n",
    "    #Implement mask on our data to avoid those probabilities. \n",
    "#     with pyro.poutine.mask(mask=mask):\n",
    "#         X = pyro.sample(\"X\", dist.Poisson(F @ G), obs=data)x\n",
    "    X = pyro.sample(\"X\", dist.Poisson(F @ G).mask(mask), obs=data)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def poisson_gamma_dynamical_system(data, latent_dim, mask=None):\n",
    "    # Define model parameters\n",
    "    n, p = data.shape\n",
    "    time_steps = n\n",
    "    G_matrix = []\n",
    "\n",
    "    # Priors for latent states\n",
    "    alpha = pyro.sample(\"alpha\", dist.Gamma(1., 1.))\n",
    "    beta = pyro.sample(\"beta\", dist.Gamma(1., 1.))\n",
    "    \n",
    "    g_previous = pyro.sample(\"g_0\", dist.Gamma(alpha, beta).expand([latent_dim]))\n",
    "\n",
    "    #F should have shape latent * p \n",
    "    F_matrix = pyro.sample(\"F\", dist.Gamma(1., 1.).expand([latent_dim, p]))\n",
    "    \n",
    "    G_matrix.append(g_previous)\n",
    "    \n",
    "    \n",
    "    #K by K transition matrix\n",
    "    concentration = torch.tensor([1] * latent_dim, dtype=torch.float)\n",
    "    \n",
    "\n",
    "    transition_matrix = pyro.sample(\"transition_matrix\", dist.Dirichlet(concentration).expand([latent_dim]))#K by K matrix, Dirichlet Prior\n",
    "\n",
    "    # Transition dynamics for the latent states\n",
    "    \n",
    "    # Transition dynamics for the latent states\n",
    "    for t in pyro.markov(range(1, time_steps)):\n",
    "        alpha_t = transition_matrix @ g_previous.T # previous\n",
    "        \n",
    "        g_t = pyro.sample(f\"g_{t}\", dist.Gamma(alpha_t, beta))\n",
    "        G_matrix.append(g_t)\n",
    "\n",
    "        # Update the previous state\n",
    "        g_previous = g_t\n",
    "\n",
    "    G_matrix = torch.stack(G_matrix, dim=0)\n",
    "    \n",
    "    if mask is None:\n",
    "        mask = torch.ones(data.shape).bool()\n",
    "    #Apply masking if necessary \n",
    "    X = pyro.sample(\"X\", dist.Poisson(G_matrix @ F_matrix).mask(mask), obs=data)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the MCMC functions we use to sample from the posterior distribution for posterior inference for each\n",
    "of our three models. We use the NUTS kernel and pyro's built in Monte Carlo Markov Chain process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_pca_mcmc(data, latent_dim, warmup_steps, num_samples):\n",
    "    #data = torch.tensor(data.values)\n",
    "    pyro.clear_param_store()   \n",
    "    kernel = mcmc.NUTS(bayesian_pca)\n",
    "    mcmc_run = mcmc.MCMC(kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "\n",
    "    N, P = data.shape\n",
    "    mask_rows = torch.randperm(N)[:20]\n",
    "    mask_cols = torch.randperm(P)[:5]\n",
    "\n",
    "    mask = torch.ones(N,P)\n",
    "\n",
    "    for i in mask_rows:\n",
    "        for j in mask_cols:\n",
    "            mask[i,j] = 0\n",
    "    random_mask = mask.bool() #Boolean Tensor for mask\n",
    "\n",
    "    end_block_mask = torch.ones(N,P)\n",
    "    end_block_mask[-30:, -3:] = 0\n",
    "    end_block_mask = end_block_mask.bool()\n",
    "\n",
    "\n",
    "    test_mask = torch.ones(N,P)\n",
    "    test_mask = test_mask.bool()\n",
    "\n",
    "    #Apply MCMC to our data\n",
    "    mcmc_run.run(data, latent_dim, random_mask)\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "    #print(dummy_train)\n",
    "    \n",
    "    posterior_samples = mcmc_run.get_samples()\n",
    "\n",
    "    # Extract W, sigma, and Z samples\n",
    "    W_samples = posterior_samples[\"W\"]\n",
    "    sigma_samples = posterior_samples[\"sigma\"]\n",
    "    Z_samples = posterior_samples[\"Z\"]\n",
    "    \n",
    "    return W_samples, sigma_samples, Z_samples\n",
    "\n",
    "def poisson_mcmc(training_data, latent_dim, warmup_steps, samples, mask_type=\"Random\"): #Default masking type is Random\n",
    "    #Runs MCMC on the poisson model, returns F and G Samples\n",
    "\n",
    "    #training_data = torch.tensor(training_data.values)\n",
    "    #Is our latent dimension here also deterined as a hyperparameter? \n",
    "    model = poisson_factorization\n",
    "\n",
    "\n",
    "    #Create mask for training/testing purposes\n",
    "    N, P = training_data.shape\n",
    "    \n",
    "    if mask_type == \"Random\":\n",
    "        mask_rows = torch.randperm(N)[:20]\n",
    "        mask_cols = torch.randperm(P)[:5]\n",
    "\n",
    "        mask = torch.ones(N,P)\n",
    "\n",
    "        for i in mask_rows:\n",
    "            for j in mask_cols:\n",
    "                mask[i,j] = 0\n",
    "\n",
    "        mask = mask.bool() #Random column and rows mask\n",
    "        \n",
    "    elif mask_type == \"End\":\n",
    "\n",
    "        #TEST END BLOCK MASK\n",
    "        end_block_mask = torch.ones(N,P)\n",
    "        end_block_mask[-30:, -3:] = 0\n",
    "        mask = end_block_mask.bool()\n",
    "        \n",
    "    elif mask_type == \"None\":\n",
    "        mask = torch.ones(N,P).bool()\n",
    "    else:\n",
    "        print(\"Invalid Mask\")\n",
    "        return\n",
    "\n",
    "    # Run MCMC\n",
    "    kernel = mcmc.NUTS(model)\n",
    "    mcmc_run = mcmc.MCMC(kernel, num_samples=samples, warmup_steps=warmup_steps)\n",
    "\n",
    "    #Run MCMC process on our data with given Latent dimension\n",
    "    mcmc_run.run(training_data, latent_dim, mask)\n",
    "    \n",
    "    posterior_samples = mcmc_run.get_samples()\n",
    "\n",
    "    # Extract F, G, lambda_f, and lambda_g samples\n",
    "    F_samples = posterior_samples[\"F\"]\n",
    "    G_samples = posterior_samples[\"G\"]\n",
    "    \n",
    "    return F_samples, G_samples\n",
    "    \n",
    "    \n",
    "def poisson_gamma_mcmc(training_data, latent_dim, warmup_steps, samples, mask_type = None):\n",
    "    \n",
    "    model = poisson_gamma_dynamical_system\n",
    "    \n",
    "    N,P = training_data.shape\n",
    "    \n",
    "    kernel = mcmc.NUTS(model)\n",
    "    \n",
    "    pyro.clear_param_store()\n",
    "    mcmc_run = mcmc.MCMC(kernel, num_samples = samples, warmup_steps = warmup_steps)\n",
    "    \n",
    "    mcmc_run.run(training_data, latent_dim)\n",
    "    \n",
    "    posterior_samples = mcmc_run.get_samples()\n",
    "    \n",
    "    F_samples = posterior_samples[\"F\"]\n",
    "    \n",
    "    \n",
    "    G_samples = [posterior_samples[f\"g_{t}\"].numpy() for t in range(N)]\n",
    "    G_samples = np.stack(G_samples, axis=1)\n",
    "        \n",
    "    return G_samples, F_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our posterior check process, with both a posterior predictive check and a population predictive check. \n",
    "\n",
    "The population predictive check compares the likelihood of the current data samples of the produced probability distribution\n",
    "to the likelihood of simulated samples from the produced probability distribution. \n",
    "\n",
    "\n",
    "The posterior predictive check is more robust, and rather masks out a portion of the data during the training process to be held out\n",
    "for testing. For this masked part, we compare likelihood of the data samples that weren't used in training to likelihoods produced from \n",
    "simulating samples from the posterior probability distribution. \n",
    "\n",
    "We use 2 kinds of mask, a random mask, that creates a random combination of counties and timestamps to mask out, as well as\n",
    "an end block mask, that masks out a lower rectangle, simulating holding out data for the last few time periods for a certain amount of \n",
    "counties, which better fits our task in time inference, as it is using past data to predict future data in a sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Population Predictive Check function\n",
    "\n",
    "def population_predictive_check(reconstructed_mean, mask, data, model, variance=None): #Model Refers to PCA, Poisson, Etc\n",
    "    log_prob = []\n",
    "    test_prob_list = []\n",
    "\n",
    "    for mean in reconstructed_mean:\n",
    "        testing_mean = torch.tensor(mean[mask])\n",
    "        \n",
    "        if (model == 'Poisson'):\n",
    "            sample_dist = dist.Poisson(testing_mean)\n",
    "            \n",
    "        elif (model == 'PCA'):\n",
    "            sample_dist = dist.Normal(testing_mean, variance)\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid model choice\")\n",
    "            return\n",
    "\n",
    "        #Only have prediction on selected mask_rows and mask_cols\n",
    "        y_pred  = sample_dist.sample()\n",
    "\n",
    "        #Calculate the likelihood given this sample\n",
    "        y_pred_likelihood = sample_dist.log_prob(y_pred).sum()\n",
    "\n",
    "        #Now, we want to calculate the likelihood of the actual data. \n",
    "        data_test = torch.tensor(data[mask])\n",
    "\n",
    "        test_prob = sample_dist.log_prob(data_test).sum()\n",
    "\n",
    "        log_prob.append(-1 * y_pred_likelihood) \n",
    "        test_prob_list.append(-1 * test_prob)\n",
    "\n",
    "\n",
    "    count = sum([1 for x, y in zip(log_prob, test_prob_list) if x > y]) #Count is if sample > data. \n",
    "    percent_likelihood = count / len(log_prob)\n",
    "\n",
    "    print(\"Percentage of Test distribution more likely than Y_Pred is\" + str(percent_likelihood))\n",
    "    \n",
    "    print(log_prob[:5])\n",
    "    print(test_prob_list[:5])\n",
    "    \n",
    "    return percent_likelihood\n",
    "    \n",
    "    \n",
    "#General Posterior Predictive Check Method(No Mask)\n",
    "\n",
    "def posterior_predictive_check(reconstructed_mean, data, model, variance=None): #Model Refers to PCA, Poisson, Etc\n",
    "    log_prob = []\n",
    "    test_prob_list = []\n",
    "\n",
    "    for mean in reconstructed_mean:\n",
    "        if (model == 'Poisson'):\n",
    "            sample_dist = dist.Poisson(mean)\n",
    "            \n",
    "        elif (model == 'PCA'):\n",
    "            sample_dist = dist.Normal(mean, variance)\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid model choice\")\n",
    "            return\n",
    "\n",
    "        #Only have prediction on selected mask_rows and mask_cols\n",
    "        y_pred = sample_dist.sample()\n",
    "\n",
    "        #Calculate the likelihood given this sample\n",
    "        y_pred_likelihood = sample_dist.log_prob(y_pred).sum()\n",
    "\n",
    "        #Now, we want to calculate the likelihood of the actual data. \n",
    "        data_test = torch.tensor(data)\n",
    "\n",
    "        test_prob = sample_dist.log_prob(data_test).sum()\n",
    "\n",
    "        log_prob.append(-1 * y_pred_likelihood) \n",
    "        test_prob_list.append(-1 * test_prob)\n",
    "\n",
    "\n",
    "    count = sum([1 for x, y in zip(log_prob, test_prob_list) if x > y]) #Count is if sample > data. \n",
    "    percent_likelihood = count / len(log_prob)\n",
    "\n",
    "    print(\"Percentage of Test distribution more likely than Y_Pred is\" + str(percent_likelihood))\n",
    "    \n",
    "    print(log_prob[:5])\n",
    "    print(test_prob_list[:5])\n",
    "    \n",
    "    return percent_likelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when our model does pass the predictive check, we implement the actual synthetic control process. \n",
    "\n",
    "In this process, we take the trained samples that is indexed by counties (F_samples for Poisson, W_samples for PCA) ,\n",
    "and perform a large scale regression using these samples. \n",
    "\n",
    "We regress the post intervention time stamp results on our X, which is a concatenation of the Trained samples and the \n",
    "actual data for the donor counties, and a A column that represents whether or not the county opened to fans that is 1 for the stadium county. \n",
    "\n",
    "To produce the synthetic control coutnerfactuals, we change the A from 1 to 0 for the stadium county, and then reconstruct \n",
    "the data with the new regression weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def Synthetic_control(W_samples, donor_train, stadium_train, donor_test, stadium_test, dates, team, RBSC, regression_type = 'Ridge'):\n",
    "    #W_Samples should be size latent_dim * p\n",
    "    \n",
    "    #Z_samples size n * latent_dim\n",
    "    #Donor_data should be size n*p \n",
    "    \n",
    "    #Stadium_data should be size n*1\n",
    "    \n",
    "    #Our Y should be donor_test and stadium_test, so size should be post_intervention_t * p+1\n",
    "    #Stadium_test is our Y, shape p by T\n",
    "    \n",
    "    synthetic_control_paths = []\n",
    "    pre_intervention_days, counties = donor_train.shape\n",
    "    post_intervention_days, counties = donor_test.shape\n",
    "\n",
    "    latent_dim, w_counties = W_samples[0].shape\n",
    "    \n",
    "    #Store results for different synthetic control regressions, as we iterate through W_samples\n",
    "    counterfactual_outcomes_deconfound = []\n",
    "    time_step_outcomes = []\n",
    "    \n",
    "    for W_sample in W_samples: \n",
    "    \n",
    "        #Run Regression with latent_p W_sample on Y_it, for each timestep t post intervention. \n",
    "        \n",
    "        \n",
    "        #Add [1] if it is intervention\n",
    "        \n",
    "        A = torch.zeros(w_counties)\n",
    "        A[-1] = 1\n",
    "        A = A.view(1, w_counties)\n",
    "\n",
    "        x_regressors = torch.cat((W_sample, A), dim=0).transpose(0,1)\n",
    "        \n",
    "        assert x_regressors.shape == (w_counties, latent_dim + 1)\n",
    "\n",
    "        #For Yit ~ Ai + Yi(-m) + … + Yi(-1), ,we need stadium data pre intervention, which is just stadium train\n",
    "\n",
    "        #We want to fit donor_test and stadium_test together in one regression\n",
    "        stadium_test = stadium_test.view(-1, 1)\n",
    "\n",
    "        Y = torch.cat((donor_test, stadium_test), dim = 1)\n",
    "        \n",
    "        \n",
    "        assert Y.shape == (post_intervention_days,  w_counties)\n",
    "        \n",
    "        #Ridge regression with alpha = 0.001 Works better\n",
    "        #Try Ridge, Lasso, MLP Regression ,with Cross Validation to find best value. \n",
    "        \n",
    "        #Parameters for GridsearchCV for Lasso and Ridge\n",
    "        parameters = {\"alpha\": [0, 1e-4,1e-3, 1e-2]}\n",
    "        if regression_type == 'Ridge':\n",
    "            reg = linear_model.Ridge()\n",
    "            clf = GridSearchCV(reg, parameters, scoring='r2', cv=5)\n",
    "        elif regression_type == 'MLP': \n",
    "            clf = MLPRegressor(hidden_layer_sizes=(20, 20), max_iter=5000)\n",
    "        elif regression_type == 'Lasso':\n",
    "            reg = linear_model.Lasso()\n",
    "            parameters = {'alpha': [0.01]}\n",
    "            #Right now 0.01 working best for lasso\n",
    "            clf = GridSearchCV(reg, parameters, scoring = 'r2', cv=5)\n",
    "            \n",
    "        deconfound_lin_reg = clf.fit(x_regressors, Y.transpose(0,1))\n",
    "        deconfound_lin_reg = clf.best_estimator_\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        counterfactual_x = x_regressors\n",
    "        counterfactual_x[w_counties-1, latent_dim] = 0 #Set \"stadium open status\" to 0.\n",
    "        \n",
    "        if regression_type == 'MLP':\n",
    "            counterfactual_outcome = deconfound_lin_reg.predict(counterfactual_x)\n",
    "        elif regression_type in ['Ridge', 'Lasso']:\n",
    "            coefficients = torch.tensor(deconfound_lin_reg.coef_)\n",
    "        \n",
    "            assert coefficients.shape[1] == latent_dim + 1\n",
    "            assert coefficients.shape[0] == post_intervention_days\n",
    "\n",
    "\n",
    "            #Now, we perform reconstruction for the deconfounded counterfactual result by setting A=0 for the \n",
    "            #stadium county .\n",
    "\n",
    "            counterfactual_outcome = counterfactual_x @ torch.tensor(coefficients).transpose(0,1)\n",
    "        \n",
    "        assert counterfactual_outcome.shape == Y.transpose(0,1).shape\n",
    "        \n",
    "        counterfactual_outcomes_deconfound.append(counterfactual_outcome)\n",
    "        \n",
    "        \n",
    "        stadium_train = stadium_train.view(-1,1)\n",
    "\n",
    "        \n",
    "    \n",
    "    if regression_type in ['Ridge', 'Lasso']:\n",
    "        stadium_cf_deconfound = [counterfactuals[-1, :].numpy() for counterfactuals in counterfactual_outcomes_deconfound]\n",
    "    else:\n",
    "        stadium_cf_deconfound = [counterfactuals[-1, :] for counterfactuals in counterfactual_outcomes_deconfound]\n",
    "    \n",
    "    \n",
    "    percentiles_5 = np.percentile(stadium_cf_deconfound, 5, axis=0)\n",
    "    percentiles_95 = np.percentile(stadium_cf_deconfound, 95, axis=0)\n",
    "    \n",
    "    \n",
    "    median_values = np.median(stadium_cf_deconfound, axis=0)\n",
    "    post_dates = dates[-post_intervention_days:]\n",
    "    plt.fill_between(post_dates, percentiles_5, percentiles_95, color='gray', alpha=0.3, label='5th-95th Percentile Range')\n",
    "    pre_dates = dates[:pre_intervention_days]\n",
    "    plt.plot(post_dates, median_values, label='Bayesian Synthetic Control Median Values ' + str(regression_type) + ' '+ str(team))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tick_params(axis='x', which='major')\n",
    "    \n",
    "    plt.plot(post_dates, stadium_test, label = 'Stadium Data Postintervention')\n",
    "    plt.plot(pre_dates, stadium_train, label = 'Stadium Data Pre-intervention')\n",
    "    plt.axvline(x=dates[pre_intervention_days], color='grey')\n",
    "\n",
    "    \n",
    "    plt.plot(dates, RBSC, label = 'Robust Synthetic Control')\n",
    "    plt.title('Synthetic Control ' + str(team) + \" with number of latents \" + str(W_samples[0].shape[0]) + \" and counties \" + str(W_samples[0].shape[1]) +\" and number of samples \" + str(len(W_samples)))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return counterfactual_outcomes_deconfound\n",
    "\n",
    "# def Poisson_synthetic_control(F_samples, donor_train, stadium_train, donor_test, stadium_test, dates, team, RBSC, regression_type = 'Ridge'):\n",
    "#     #F_samples should have shape latent_dim * donors\n",
    "\n",
    "    \n",
    "#     #Same thing as PCA synthetic control, except we use F_samples as deconfounding latents. \n",
    "#     counterfactuals = PCA_synthetic_control(F_samples, donor_train, stadium_train, donor_test, stadium_test, dates, team, RBSC, regression_type)\n",
    "    \n",
    "#     return counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary for latent space dimensions for each team. \n",
    "latent_dim_dict = {'Arizona': 5, 'Atlanta':5 , 'Baltimore':15, 'Buffalo':25, 'Carolina': 5, 'Chicago':15, 'Cincinnati':13, 'Cleveland':1, 'Dallas': 37, 'Denver':5 , 'Detroit':12, 'Green Bay': 52, 'Houston': 12, 'Indianapolis': 10, 'Jacksonville':12, 'Kansas City':11, 'Las Vegas':12, 'LA Chargers':5,'LA Rams':8, 'Miami':8, 'Minnesota':5, 'New England': 9, 'New Orleans':18 , 'NY Giants':7,'NY Jets':7, 'Philadelphia':10, 'Pittsburgh': 25, 'San Francisco': 10, 'Seattle': 14, 'Tampa Bay': 15, 'Tennessee': 13, 'Washington':4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample:  59%|█████▊    | 1757/3000 [1:42:45,  3.50s/it, step size=5.87e-04, acc. prob=0.930]"
     ]
    }
   ],
   "source": [
    "zipped_input = zip(grouped_df['Team'], grouped_df['County(s)'], grouped_df['State'], grouped_df['First date home stadium open to fans'])\n",
    "import ast\n",
    "\n",
    "def bayesian_SC_graph(team_county_data):\n",
    "    count = 0\n",
    "    for i, (team, county, state, date) in enumerate(team_county_data):\n",
    "        count += 1\n",
    "        date = ast.literal_eval(date)\n",
    "        #team = ast.literal_eval(team)\n",
    "        county = ast.literal_eval(county)\n",
    "        state = ast.literal_eval(state)\n",
    "        if team in ['San Francisco']:  #Ignore Arizona\n",
    "            #x are donors, y are stadiums, train is pre-intervention, test is post intervention\n",
    "\n",
    "        \n",
    "            x_train, y_train, x_test, y_test, dates = get_training_data(team,county ,state, date, True, 0)\n",
    "\n",
    "            x_train, y_train, x_test, y_test = torch.tensor(x_train.values),torch.tensor(y_train.values), torch.tensor(x_test.values), torch.tensor(y_test.values)\n",
    "\n",
    "\n",
    "            #Combine Donor and Stadium in our traning data\n",
    "            synthetic_control_training = torch.cat((x_train, torch.unsqueeze(y_train,1)), dim=1)\n",
    "\n",
    "            #Produce Confounder variable to use in our regression\n",
    "\n",
    "            #Run Poisson Factorization Model\n",
    "\n",
    "            latent_dimension = latent_dim_dict[team]\n",
    "            \n",
    "\n",
    "\n",
    "            f_samples, g_samples = poisson_mcmc(synthetic_control_training, latent_dimension, 1500,1500, \"None\")\n",
    "\n",
    "#             np.save(f'{team}_g_samples.npy', g_samples.numpy())\n",
    "#             np.save(f'{team}_f_samples.npy', f_samples.numpy())\n",
    "            synthetic_control_training = synthetic_control_training.clamp(min=0)\n",
    "            \n",
    "            \n",
    "            N,P = synthetic_control_training.shape\n",
    "            end_block_mask = torch.ones(N,P)\n",
    "            end_block_mask[-30:, -3:] = 0 #The mask size can be edited\n",
    "            end_block_mask = end_block_mask.bool()\n",
    "\n",
    "            population_check = population_predictive_check(f_samples @ g_samples, end_block_mask, synthetic_control_training.round().to(torch.int64), 'Poisson')\n",
    "\n",
    "        \n",
    "            poisson_p_value = posterior_predictive_check(f_samples @ g_samples, synthetic_control_training.round().to(torch.int64),'Poisson')\n",
    "\n",
    "            poisson_posterior_check = poisson_p_value >= 0\n",
    "            \n",
    "            RBSC_results = np.load(f'RBSC_Replications/{team}_RBSC.npy')\n",
    "            \n",
    "            if poisson_posterior_check:\n",
    "\n",
    "                deconfounders_ridge = Synthetic_control(g_samples, x_train, y_train, x_test, y_test, dates, team, RBSC_results, regression_type='Ridge')\n",
    "                #deconfounders_lasso = Synthetic_control(g_samples, x_train, y_train, x_test, y_test, dates, team, RBSC_results, regression_type='Lasso')\n",
    "\n",
    "            else:\n",
    "                w_samples, sigma_samples, z_samples = bayesian_pca_mcmc(synthetic_control_training, 10, 1000, 800)\n",
    "\n",
    "                pca_p_value = posterior_predictive_check(z_samples@w_samples, synthetic_control_training, 'PCA', variance=sigma_samples[0])\n",
    "\n",
    "                #With these samples, perform a POSTERIOR CHECK first, if it passes then we use that model. \n",
    "\n",
    "                pca_posterior_check = pca_p_value>=0\n",
    "\n",
    "                if pca_posterior_check:\n",
    "                    deconfounders = Synthetic_control(w_samples, x_train, y_train, x_test, y_test, dates, team, RBSC_results, regression_type='Ridge')\n",
    "\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "bayesian_SC_graph(zipped_input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_input = zip(grouped_df['Team'], grouped_df['County(s)'], grouped_df['State'], grouped_df['First date home stadium open to fans'])\n",
    "\n",
    "def bayesian_SC_graph_pre_loaded(team_county_data, calculate_ATT = False): #Difference is this function takes from pre-trained samples.\n",
    "#We want to calculate ATT. \n",
    "\n",
    "    ATT_Bayesian_list = []\n",
    "    ATT_RBSC_list = []\n",
    "    \n",
    "    for i, (team, county, state, date) in enumerate(team_county_data):\n",
    "        #if team in ['Cincinnati', 'Cleveland', 'Carolina', 'Atlanta', 'Denver', 'Kansas City', 'Minnesota', 'Philadelphia', 'Seattle', 'San Francisco', 'Tennessee']:\n",
    "        if team != 'Arizona': #Arizona data not included\n",
    "            #f_samples = torch.from_numpy(np.load(f'{team}_f_samples.npy')) F samples used for posterior check\n",
    "            g_samples = torch.from_numpy(np.load(f'Trained_Samples/{team}_g_samples.npy'))\n",
    "            #f_samples = torch.from_numpy(np.load(f'Trained_Samples/{team}_f_samples.npy'))\n",
    "\n",
    "            x_train, y_train, x_test, y_test, dates = get_training_data(team,county ,state, date, True, 0)\n",
    "\n",
    "            x_train, y_train, x_test, y_test = torch.tensor(x_train.values),torch.tensor(y_train.values), torch.tensor(x_test.values), torch.tensor(y_test.values)\n",
    "            synthetic_control_training = torch.cat((x_train, torch.unsqueeze(y_train,1)), dim=1)\n",
    "            synthetic_control_training = synthetic_control_training.clamp(min=0)\n",
    "\n",
    "            synthetic_control_training = synthetic_control_training.clamp(min=0)\n",
    "            RBSC_results = create_synthetic_graph(team, county, state, date, False, -1)\n",
    "\n",
    "            deconfounders_ridge = Poisson_synthetic_control(g_samples, x_train, y_train, x_test, y_test, dates, team, RBSC_results, regression_type='Ridge', save_fig = True)\n",
    "            #deconfounders_lasso = Poisson_synthetic_control(g_samples, x_train, y_train, x_test, y_test, dates, team, RBSC_results, regression_type='Lasso')\n",
    "            \n",
    "            #post_check = posterior_predictive_check(f_samples @ g_samples, synthetic_control_training.round().to(torch.int64),'Poisson')\n",
    "            \n",
    "            if calculate_ATT:\n",
    "                ATT_RBSC_list.append(RBSC_results)\n",
    "                ATT_Bayesian_list.append(deconfounders_ridge)\n",
    "                \n",
    "#             posterior_check = posterior_predictive_check(f_samples @ g_samples, synthetic_control_training.round().to(torch.int64),'Poisson')\n",
    "#             print(posterior_check)\n",
    "    \n",
    "#     min_len = min(len(lst) for lst in ATT_Bayesian_list)\n",
    "#     truncated_lists = [lst[:min_len] for lst in ATT_Bayesian_list]\n",
    "#     Bayesian_transposed= np.array(truncated_lists).T\n",
    "#     ATT_Bayesian = np.mean(Bayesian_transposed, axis=0)\n",
    "    \n",
    "#     min_len = min(len(lst) for lst in ATT_bayesian_list)\n",
    "#     truncated_lists = [lst[:min_len] for lst in ATT_Bayesian_list]\n",
    "#     RBSC_transposed= np.array(truncated_lists).T\n",
    "#     ATT_RBSC = np.mean(Bayesian_transposed, axis=0)\n",
    "    \n",
    "    return 1,2# ATT_Bayesian, ATT_RBSC\n",
    "            \n",
    "ATT_Bayesian, Att_RBSC = bayesian_SC_graph_pre_loaded(zipped_input, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
