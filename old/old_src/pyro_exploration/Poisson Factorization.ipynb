{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro.infer.mcmc as mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aefef21208fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Generate fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoisson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Convert data to PyTorch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def poisson_factorization(data, latent_dim):\n",
    "    # Define model parameters\n",
    "    n, p = data.shape\n",
    "    \n",
    "    #Construct samples of F, G, and X. X is assumed to be poisson distribution of F * G\n",
    "    F = pyro.sample(\"F\", dist.Gamma(1., 1.).expand([n, latent_dim]))\n",
    "    G = pyro.sample(\"G\", dist.Gamma(1., 1.).expand([latent_dim, p]))\n",
    "    \n",
    "    \n",
    "    # Define masking function to hide lower right corner of the data\n",
    "    mask = torch.ones_like(data)\n",
    "    mask[-20:, -2:] = 0.\n",
    "    mask = mask.bool()\n",
    "    \n",
    "    # Observe the observed entries of X\n",
    "    pyro.sample(\"X_observed\", dist.Poisson((F @ G)[mask]), obs=torch.tensor(data[mask]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    X = F @ G\n",
    "    return X\n",
    "\n",
    "# Define model\n",
    "\n",
    "#Is our latent dimension here also deterined as a hyperparameter? \n",
    "latent_dim = 2\n",
    "model = poisson_factorization\n",
    "\n",
    "# Generate fake data\n",
    "n, p = 50, 5\n",
    "data = np.random.poisson(5., size=(n, p)) + 500\n",
    "\n",
    "# Convert data to PyTorch tensor\n",
    "data = torch.tensor(data)\n",
    "\n",
    "# Run MCMC\n",
    "num_samples = 1000\n",
    "warmup_steps = 1000\n",
    "kernel = mcmc.NUTS(model)\n",
    "mcmc_run = mcmc.MCMC(kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "\n",
    "#Run MCMC process on our data with given Latent dimension\n",
    "mcmc_run.run(data, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[515.2449, 504.5264, 503.0182, 500.1896, 502.1837],\n",
       "        [492.8720, 488.6221, 490.4343, 488.7366, 491.2715],\n",
       "        [501.2274, 497.2758, 499.3195, 497.6552, 500.2719],\n",
       "        [505.5141, 502.0925, 504.4592, 502.8754, 505.5734],\n",
       "        [501.7811, 496.6788, 498.1031, 496.2445, 498.7442],\n",
       "        [488.7137, 485.5200, 487.8701, 486.3580, 488.9782],\n",
       "        [493.0889, 488.5482, 490.2044, 488.4574, 490.9633],\n",
       "        [504.1041, 500.4686, 502.7076, 501.0907, 503.7578],\n",
       "        [524.2792, 518.2939, 519.4275, 517.3757, 519.9191],\n",
       "        [491.9086, 487.2018, 488.7582, 486.9857, 489.4672],\n",
       "        [514.0132, 509.4611, 511.2859, 509.4953, 512.1265],\n",
       "        [518.5750, 512.3624, 513.3251, 511.2466, 513.7318],\n",
       "        [516.6780, 512.5861, 514.6827, 512.9640, 515.6594],\n",
       "        [520.2710, 514.5070, 515.7271, 513.7204, 516.2628],\n",
       "        [527.9942, 522.1348, 523.3676, 521.3295, 523.9086],\n",
       "        [494.0881, 484.8686, 483.9963, 481.4617, 483.4846],\n",
       "        [498.9581, 495.4121, 497.6568, 496.0651, 498.7105],\n",
       "        [528.3599, 519.6418, 519.3278, 516.8090, 519.0914],\n",
       "        [500.9129, 496.1935, 497.8182, 496.0256, 498.5601],\n",
       "        [506.2020, 499.1830, 499.6051, 497.4158, 499.7419],\n",
       "        [525.3652, 517.2060, 517.1698, 514.7508, 517.0736],\n",
       "        [491.6750, 487.4755, 489.3050, 487.6182, 490.1512],\n",
       "        [496.5453, 489.2515, 489.4439, 487.2277, 489.4667],\n",
       "        [506.7916, 502.0098, 503.6497, 501.8350, 504.3985],\n",
       "        [510.5227, 505.3331, 506.7832, 504.8925, 507.4359],\n",
       "        [498.4652, 494.5570, 496.6010, 494.9496, 497.5540],\n",
       "        [516.5721, 512.1376, 514.0474, 512.2714, 514.9304],\n",
       "        [503.2483, 499.5970, 501.8203, 500.2024, 502.8626],\n",
       "        [508.6978, 502.8462, 503.9219, 501.9237, 504.3869],\n",
       "        [521.0128, 514.9873, 516.0718, 514.0198, 516.5392],\n",
       "        [505.5668, 501.3120, 503.2276, 501.5037, 504.1149],\n",
       "        [517.8654, 511.2917, 512.0526, 509.9147, 512.3579],\n",
       "        [499.6123, 492.4308, 492.7098, 490.5064, 492.7757],\n",
       "        [510.6684, 506.0195, 507.7639, 505.9637, 508.5645],\n",
       "        [495.7389, 491.7841, 493.7802, 492.1264, 494.7095],\n",
       "        [503.1982, 496.5754, 497.1873, 495.0706, 497.4200],\n",
       "        [496.9881, 493.2499, 495.3738, 493.7538, 496.3671],\n",
       "        [501.9434, 496.1429, 497.1901, 495.2139, 497.6416],\n",
       "        [528.4169, 522.9715, 524.4322, 522.4628, 525.0876],\n",
       "        [498.5577, 494.8516, 497.0060, 495.3883, 498.0145],\n",
       "        [514.2291, 510.0537, 512.0845, 510.3567, 513.0285],\n",
       "        [492.4968, 488.9793, 491.1853, 489.6114, 492.2206],\n",
       "        [491.5593, 488.1909, 490.4700, 488.9230, 491.5422],\n",
       "        [515.7792, 512.1168, 514.4388, 512.7940, 515.5289],\n",
       "        [521.6586, 515.1617, 515.9959, 513.8634, 516.3376],\n",
       "        [501.8041, 497.5758, 499.4742, 497.7623, 500.3535],\n",
       "        [495.7599, 490.1189, 491.2009, 489.2638, 491.6709],\n",
       "        [507.7473, 503.5584, 505.5278, 503.8107, 506.4420],\n",
       "        [488.0434, 482.7845, 484.0091, 482.1516, 484.5520],\n",
       "        [497.2494, 493.9098, 496.2520, 494.6984, 497.3550]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract posterior samples\n",
    "posterior_samples = mcmc_run.get_samples()\n",
    "\n",
    "# Extract F, G, lambda_f, and lambda_g samples\n",
    "F_samples = posterior_samples[\"F\"]\n",
    "G_samples = posterior_samples[\"G\"]\n",
    "\n",
    "\n",
    "# Plot posterior samples of F and G\n",
    "# fig, axs = plt.subplots(latent_dim, 2, figsize=(12, 6), sharex=True)\n",
    "# for i in range(latent_dim):\n",
    "#     axs[i, 0].hist(F_samples[:, i, :], bins=50)\n",
    "#     axs[i, 0].set_title(f\"F[:, {i+1}]\")\n",
    "#     axs[i, 1].hist(G_samples[i, :, :], bins=50)\n",
    "#     axs[i, 1].set_title(f\"G[{i+1}, :]\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "F_samples[-1] @ G_samples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[507, 504, 504, 505, 507],\n",
       "        [503, 506, 502, 510, 508],\n",
       "        [501, 504, 505, 502, 504],\n",
       "        [504, 506, 508, 506, 503],\n",
       "        [505, 504, 506, 504, 506],\n",
       "        [504, 505, 503, 504, 507],\n",
       "        [504, 507, 501, 507, 501],\n",
       "        [501, 504, 505, 507, 507],\n",
       "        [500, 506, 504, 504, 508],\n",
       "        [507, 511, 500, 502, 506],\n",
       "        [504, 505, 507, 500, 503],\n",
       "        [507, 507, 503, 504, 503],\n",
       "        [504, 505, 505, 506, 506],\n",
       "        [506, 502, 505, 506, 507],\n",
       "        [508, 503, 508, 504, 508],\n",
       "        [502, 506, 502, 505, 504],\n",
       "        [503, 506, 503, 507, 504],\n",
       "        [503, 504, 504, 503, 508],\n",
       "        [503, 506, 502, 506, 508],\n",
       "        [505, 506, 503, 504, 503],\n",
       "        [502, 508, 506, 505, 505],\n",
       "        [502, 502, 505, 506, 504],\n",
       "        [503, 506, 504, 503, 504],\n",
       "        [507, 506, 505, 501, 507],\n",
       "        [506, 503, 508, 504, 503],\n",
       "        [505, 502, 504, 500, 506],\n",
       "        [507, 504, 506, 505, 505],\n",
       "        [503, 504, 501, 504, 507],\n",
       "        [503, 503, 506, 506, 505],\n",
       "        [504, 505, 502, 508, 506],\n",
       "        [505, 505, 502, 508, 505],\n",
       "        [506, 507, 508, 505, 502],\n",
       "        [509, 508, 506, 508, 504],\n",
       "        [505, 504, 504, 508, 504],\n",
       "        [502, 504, 506, 510, 503],\n",
       "        [504, 508, 506, 506, 502],\n",
       "        [506, 504, 504, 508, 503],\n",
       "        [507, 504, 509, 503, 507],\n",
       "        [506, 505, 509, 503, 506],\n",
       "        [505, 503, 512, 505, 510],\n",
       "        [505, 501, 505, 509, 504],\n",
       "        [506, 503, 502, 507, 505],\n",
       "        [504, 506, 504, 508, 502],\n",
       "        [508, 506, 506, 507, 504],\n",
       "        [506, 503, 501, 507, 505],\n",
       "        [505, 504, 504, 503, 506],\n",
       "        [505, 504, 505, 507, 502],\n",
       "        [504, 505, 510, 504, 505],\n",
       "        [505, 505, 504, 505, 506],\n",
       "        [506, 507, 505, 503, 506]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5,5)\n",
    "mask = torch.ones(5,5)\n",
    "mask[-1,-1] = 0\n",
    "mask = mask.bool()\n",
    "a[mask]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
