{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro.infer.mcmc as mcmc\n",
    "import torch.distributions\n",
    "\n",
    "\n",
    "def bayesian_pca(data, latent_dim):\n",
    "    # Define model parameters\n",
    "    n, p = data.shape\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "    mu = torch.zeros(p)\n",
    "    cov = sigma * torch.eye(p)\n",
    "\n",
    "    # Mask the lower right corner of the data\n",
    "    mask = torch.ones(n, p)\n",
    "    mask[-20:, -2:] = 0\n",
    "    masked_data = data * mask\n",
    "    \n",
    "\n",
    "    # Define the custom mask distribution\n",
    "    def mask_dist(mean, covariance):\n",
    "        masked_mean = mask * mean\n",
    "        return dist.MultivariateNormal(masked_mean, covariance_matrix=covariance)\n",
    "    \n",
    "    # Define the latent variables for the masked data\n",
    "\n",
    "    Z_mean = pyro.param(\"Z.mean\", torch.zeros(n, latent_dim))\n",
    "    Z_cov = pyro.param(\"Z.cov\", torch.eye(latent_dim))\n",
    "    Z = pyro.sample(\"Z\", dist.MultivariateNormal(Z_mean, Z_cov))\n",
    "\n",
    "    W_mean = pyro.param(\"W.mean\", torch.zeros(latent_dim, p))\n",
    "    W_cov = pyro.param(\"W.cov\", torch.eye(p))\n",
    "    W = pyro.sample(\"W\", dist.MultivariateNormal(W_mean, W_cov))\n",
    "    \n",
    "    X = pyro.sample(\"X\", mask_dist(Z @ W, cov * torch.eye(p)), obs=masked_data)\n",
    "\n",
    "    # Return the estimated latent variables\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1397.5305,   464.8815,  -811.0114, -1125.1631,   827.1343],\n",
       "        [  221.4790,    81.9377,   241.4629,   182.1595,  -764.8069],\n",
       "        [ -246.9436,    96.4646,  -385.0945,  -644.1563,  -459.4985],\n",
       "        [  466.0563,   310.5362,  -195.2516,  -365.9461,   -29.9426],\n",
       "        [ -126.3790,   420.2422,  -968.7613,   115.9764,    73.2304],\n",
       "        [  415.5353,   -32.5359,   -93.1402,  -571.9332,  -216.0164],\n",
       "        [  277.3107,     3.2673,   672.4340,    95.9534,  -500.0756],\n",
       "        [  -93.8707,  -180.2603,    71.5380,     2.6049,   267.7835],\n",
       "        [ -131.2750,  -267.8922,  -963.8295, -1092.0708,   218.3588],\n",
       "        [ -191.2837,  -594.8768,  -730.9603,  -345.2110,  -107.5722]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 300/300 [05:46,  1.16s/it, step size=1.41e-04, acc. prob=0.345]\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "\n",
    "#Number of Dimensions post PCA, hyperparameter ?\n",
    "latent_dim = 3\n",
    "model = bayesian_pca\n",
    "\n",
    "# Generate Data\n",
    "n, p = 10, 5\n",
    "data = torch.randn(n, p) * 500\n",
    "\n",
    "\n",
    "\n",
    "# Run MCMC\n",
    "num_samples = 100\n",
    "\n",
    "#Lower warmup steps for code to run\n",
    "warmup_steps = 200\n",
    "kernel = mcmc.NUTS(model)\n",
    "mcmc_run = mcmc.MCMC(kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "#Apply MCMC to our data.\n",
    "mcmc_run.run(data, latent_dim)\n",
    "pyro.clear_param_store()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 5])\n",
      "torch.Size([100, 10, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-29.6602,  14.1690,  19.4095],\n",
       "         [  1.5899,  -0.4540,   7.9255],\n",
       "         [ -1.5371,   7.6422,  -9.6393],\n",
       "         ...,\n",
       "         [  0.5550,  -6.0109,  -1.6433],\n",
       "         [-17.2170,  -0.3782, -16.2096],\n",
       "         [-16.1797, -12.7158, -15.6216]],\n",
       "\n",
       "        [[-29.6684,  14.1698,  19.3950],\n",
       "         [  1.5880,  -0.4531,   7.9247],\n",
       "         [ -1.5383,   7.6260,  -9.6396],\n",
       "         ...,\n",
       "         [  0.5554,  -6.0048,  -1.6417],\n",
       "         [-17.2175,  -0.3763, -16.2168],\n",
       "         [-16.1787, -12.6978, -15.6251]],\n",
       "\n",
       "        [[-29.6661,  14.1725,  19.3870],\n",
       "         [  1.5898,  -0.4539,   7.9259],\n",
       "         [ -1.5423,   7.6270,  -9.6385],\n",
       "         ...,\n",
       "         [  0.5542,  -6.0009,  -1.6414],\n",
       "         [-17.2239,  -0.3680, -16.2221],\n",
       "         [-16.1831, -12.6858, -15.6323]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-28.8245,  13.8792,  20.3109],\n",
       "         [  1.8854,  -0.6647,   8.2837],\n",
       "         [ -1.8405,   7.7021,  -9.7676],\n",
       "         ...,\n",
       "         [  0.4406,  -5.8224,  -1.9566],\n",
       "         [-17.8511,   0.3389, -17.2483],\n",
       "         [-16.8972, -11.7105, -17.1310]],\n",
       "\n",
       "        [[-28.7904,  13.8386,  20.2937],\n",
       "         [  1.9011,  -0.6594,   8.2715],\n",
       "         [ -1.8574,   7.7032,  -9.7651],\n",
       "         ...,\n",
       "         [  0.4365,  -5.8263,  -1.9473],\n",
       "         [-17.8831,   0.3091, -17.2229],\n",
       "         [-16.9284, -11.7467, -17.0939]],\n",
       "\n",
       "        [[-28.7977,  13.8205,  20.2703],\n",
       "         [  1.9082,  -0.6555,   8.2675],\n",
       "         [ -1.8631,   7.6947,  -9.7517],\n",
       "         ...,\n",
       "         [  0.4357,  -5.8215,  -1.9507],\n",
       "         [-17.9053,   0.2934, -17.2151],\n",
       "         [-16.9506, -11.7528, -17.0945]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract posterior samples, includes W, Z, and X. W is transformation, Z is weights, X is our data. \n",
    "#Sigma is our covariance matrix for X, assumed to be diagonal for PPCA I believe. \n",
    "posterior_samples = mcmc_run.get_samples()\n",
    "\n",
    "# Extract W, sigma, and Z samples\n",
    "W_samples = posterior_samples[\"W\"]\n",
    "sigma_samples = posterior_samples[\"sigma\"]\n",
    "Z_samples = posterior_samples[\"Z\"]\n",
    "\n",
    "print(W_samples.size())\n",
    "print(Z_samples.size())\n",
    "\n",
    "\n",
    "Z_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1397.5576,  464.9338, -811.0127,  -66.5389,  -59.8559],\n",
       "        [ 221.5226,   81.9686,  241.4845,    3.7030,    5.3524],\n",
       "        [-246.9532,   96.4928, -385.0949,  -15.7925,  -16.7233],\n",
       "        [ 466.0794,  310.5636, -195.2890,  -25.1117,  -21.8805],\n",
       "        [-126.3983,  420.1721, -968.7817,  -52.4721,  -51.9528],\n",
       "        [ 415.5706,  -32.4499,  -93.2189,   -7.5092,   -6.2844],\n",
       "        [ 277.2784,    3.2286,  672.4957,   22.9628,   25.0001],\n",
       "        [ -93.8814, -180.2111,   71.5173,   10.7733,    9.4946],\n",
       "        [-131.2433, -267.8916, -963.8311,  -26.2018,  -29.6411],\n",
       "        [-191.2900, -594.8683, -730.9649,   -3.9419,   -9.1393]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_samples[-1] @ W_samples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1397.5305,   464.8815,  -811.0114, -1125.1631,   827.1343],\n",
       "        [  221.4790,    81.9377,   241.4629,   182.1595,  -764.8069],\n",
       "        [ -246.9436,    96.4646,  -385.0945,  -644.1563,  -459.4985],\n",
       "        [  466.0563,   310.5362,  -195.2516,  -365.9461,   -29.9426],\n",
       "        [ -126.3790,   420.2422,  -968.7613,   115.9764,    73.2304],\n",
       "        [  415.5353,   -32.5359,   -93.1402,  -571.9332,  -216.0164],\n",
       "        [  277.3107,     3.2673,   672.4340,    95.9534,  -500.0756],\n",
       "        [  -93.8707,  -180.2603,    71.5380,     2.6049,   267.7835],\n",
       "        [ -131.2750,  -267.8922,  -963.8295, -1092.0708,   218.3588],\n",
       "        [ -191.2837,  -594.8768,  -730.9603,  -345.2110,  -107.5722]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n",
      "Percentage of Test distribution more likely than Y_Pred is1.0\n"
     ]
    }
   ],
   "source": [
    "reconstructed_X = Z_samples @ W_samples #Data_Rep Drawn from posterior Distribution\n",
    "\n",
    "m = torch.distributions.MultivariateNormal(torch.zeros(100,5), torch.eye(5))\n",
    "print(m.sample().size())\n",
    "\n",
    "\n",
    "log_prob = []\n",
    "test_prob_list = []\n",
    "for w,z,sig,x in zip(W_samples, Z_samples, sigma_samples, reconstructed_X):\n",
    "    #w is 2*5, z is 100*2, multiplied is 100*5. )\n",
    "    #Calculate log probability that distribution \n",
    "    \n",
    "    #Create new Y distribution based off our parameters\n",
    "    sample_dist = dist.MultivariateNormal((Z_mean@W_mean)[-20:, -2:], covariance_matrix=(torch.eye(2)*sig))\n",
    "    y_pred  = sample_dist.sample()\n",
    "    \n",
    "    #Calculate the likelihood given this sample\n",
    "    y_pred_likelihood = sample_dist.log_prob(y_pred).sum()\n",
    "    \n",
    "    #print(y_pred_likelihood)\n",
    "    \n",
    "    #Now, we want to calculate the likelihood of the actual data. \n",
    "    data_test = data[-20:, -2:,]\n",
    "    \n",
    "    test_prob = dist.MultivariateNormal((Z_mean @ W_mean)[-20:, -2:], covariance_matrix=(torch.eye(2)*sig)).log_prob(data_test).sum()\n",
    "    \n",
    "    #print(test_prob)\n",
    "\n",
    "    #break\n",
    "    \n",
    "    log_prob.append(y_pred_likelihood) \n",
    "    test_prob_list.append(test_prob)\n",
    "\n",
    "count = sum([1 for x, y in zip(log_prob, test_prob_list) if x > y])\n",
    "percent_likelihood = count / len(log_prob)\n",
    "        \n",
    "print(\"Percentage of Test distribution more likely than Y_Pred is\" + str(percent_likelihood))\n",
    "        \n",
    "\n",
    "        \n",
    "#Keep the same dimensionality, mask random values for train, then evaluate on full. \n",
    "\n",
    "#Hold out bottom corner, don't observe the outcomes. for the columns that were masked, create replicate timeseries for training, compare log probailities of replicated to log probabilities of the held out sequences. \n",
    "\n",
    "#Hold out 10 to 20 units, 2 time steps. Set up experimental design, randomly select units, last timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-90-85ffe9619cf9>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-90-85ffe9619cf9>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#Construct PPC from Posterior Sampled X, and New X from draws in Data. \n",
    "\n",
    "\n",
    "def PPC(data_new, data_obs): \n",
    "    \n",
    "    #Y_Obs is observed data we feed into the model. \n",
    "    #Y_New is observed data we don't have in the model. \n",
    "    #Y_Rep is data we draw from the posterior distribution using Y_Obs. \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x5 and 100x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-d2f3b9a5ddb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mexamplesamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mexamplesamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mrsample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_standard_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_batch_mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m_batch_mv\u001b[0;34m(bmat, bvec)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mjust\u001b[0m \u001b[0mones\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbroadcasted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x5 and 100x100)"
     ]
    }
   ],
   "source": [
    "z = torch.randn(100, 2)\n",
    "\n",
    "w = torch.randn(2, 5)\n",
    "\n",
    "sig = 5\n",
    "\n",
    "examplesamp = dist.MultivariateNormal((z@w), covariance_matrix=(torch.eye(100)*sig))\n",
    "\n",
    "examplesamp.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.size() # Original Data, 100 samples of 5 dimensions each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruct PCA data by multiplying centered data with the average weight. \n",
    "X= W @ Z\n",
    "\n",
    "\n",
    "\n",
    "W_mean = W_samples.mean(dim=0)\n",
    "pca_data = torch.matmul(data - data.mean(dim=0), W_mean.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
