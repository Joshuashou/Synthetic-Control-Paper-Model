{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer.mcmc as mcmc\n",
    "\n",
    "from path import Path\n",
    "\n",
    "latent_dim_dict = {\n",
    "    'Arizona':5, \n",
    "    'Atlanta':7 , \n",
    "    'Baltimore':15, \n",
    "    'Buffalo':25, \n",
    "    'Carolina':5, \n",
    "    'Chicago':15, \n",
    "    'Cincinnati':13, \n",
    "    'Cleveland':1, \n",
    "    'Dallas':37,\n",
    "    'Denver':5 , \n",
    "    'Detroit':12, \n",
    "    'Green Bay':52, \n",
    "    'Houston':12, \n",
    "    'Indianapolis':10, \n",
    "    'Jacksonville':12, \n",
    "    'Kansas City':11, \n",
    "    'Las Vegas':12, \n",
    "    'LA Chargers':5,\n",
    "    'LA Rams':8, \n",
    "    'Miami':8, \n",
    "    'Minnesota':5, \n",
    "    'New England':9, \n",
    "    'New Orleans':18 , \n",
    "    'NY Giants':7,\n",
    "    'NY Jets':7, \n",
    "    'Philadelphia':10, \n",
    "    'Pittsburgh':25, \n",
    "    'San Francisco':10, \n",
    "    'Seattle':14, \n",
    "    'Tampa Bay':15, \n",
    "    'Tennessee':13, \n",
    "    'Washington':4\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPCA:\n",
    "    \"\"\"Probabilistic PCA (PPCA)\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, variance_support=(0, 10)):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.variance_support = variance_support\n",
    "    \n",
    "    def likelihood(self, **kwargs):\n",
    "        H = kwargs['H']\n",
    "        Z = kwargs['Z']\n",
    "        # this indexing handles the possibility of posterior samples\n",
    "        sigma = kwargs['sigma'][..., None, None]\n",
    "        return dist.Normal(H @ Z, sigma)\n",
    "    \n",
    "    def model(self, data, mask=None):\n",
    "        n_time, n_unit = data.shape\n",
    "\n",
    "        # Sample latent factors (K x N)\n",
    "        Z = pyro.sample('Z', dist.Normal(torch.zeros(self.latent_dim, n_unit), 1.).to_event(2))\n",
    "\n",
    "        # Sample weights (T x K)\n",
    "        H = pyro.sample('H', dist.Normal(torch.zeros(n_time, self.latent_dim), 1.).to_event(2))\n",
    "\n",
    "        # Sample observed variance\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(self.variance_support[0], self.variance_support[1]))\n",
    "\n",
    "        # Sample observed data\n",
    "        with pyro.plate(\"data\"):\n",
    "            likelihood = self.likelihood(H=H, Z=Z, sigma=sigma)\n",
    "            if mask is not None:\n",
    "                likelihood = likelihood.mask(mask)\n",
    "            Y = pyro.sample(\"Y\", likelihood, obs=data)\n",
    "\n",
    "        return Y, Z, H, sigma\n",
    "\n",
    "class GAP:\n",
    "    \"\"\"The Gamma-Poisson (GaP) factor model\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, shp=1., rte=1.):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.shp = shp\n",
    "        self.rte = rte\n",
    "    \n",
    "    def likelihood(self, **kwargs):\n",
    "        return dist.Poisson(kwargs['H'] @ kwargs['Z'])\n",
    "    \n",
    "    def model(self, data, mask=None):\n",
    "        n_time, n_unit = data.shape\n",
    "        shp, rte, latent_dim = self.shp, self.rte, self.latent_dim\n",
    "\n",
    "        # Sample latent factors (K x N)\n",
    "        Z = pyro.sample(\"Z\", dist.Gamma(concentration=shp, rate=rte).expand([latent_dim, n_unit]).to_event(2))\n",
    "\n",
    "        # Sample weights (T x K)\n",
    "        H = pyro.sample(\"H\", dist.Gamma(concentration=shp, rate=rte).expand([n_time, latent_dim]).to_event(2))\n",
    "\n",
    "        # Sample observed data\n",
    "        with pyro.plate(\"data\"):\n",
    "            likelihood = self.likelihood(H=H, Z=Z)\n",
    "            if mask is not None:\n",
    "                likelihood = likelihood.mask(mask)\n",
    "            Y = pyro.sample(\"Y\", likelihood, obs=data)\n",
    "        \n",
    "        return Y, Z, H\n",
    "    \n",
    "    def gibbs_sample(self, data, mask=None, num_samples=2000, warmup_steps=2000):\n",
    "        n_time, n_unit = data.shape\n",
    "        shp, rte, latent_dim = self.shp, self.rte, self.latent_dim\n",
    "\n",
    "        data = data.detach().numpy().copy()\n",
    "        mask = mask.detach().numpy().copy()\n",
    "        data[mask] = 0\n",
    "\n",
    "        # Initialize latent factors from prior (K x N)\n",
    "        Z = np.zeros((warmup_steps + num_samples, latent_dim, n_unit))\n",
    "        Z[0] = rng.gamma(shp, 1./rte, size=(latent_dim, n_unit)) \n",
    "\n",
    "        # Sample weights from prior (T x K)\n",
    "        H = np.zeros((warmup_steps + num_samples, n_time, latent_dim))\n",
    "        H[0] = rng.gamma(shp, 1./rte, size=(n_time, latent_dim)) \n",
    "\n",
    "        rng = np.random.default_rng()\n",
    "        for s in range(1, warmup_steps + num_samples):\n",
    "            # Allocation step\n",
    "            P_TNK = np.einsum('tk,nk->tnk', H[s-1], Z[s-1])\n",
    "            P_TNK /= np.sum(P_TNK, axis=2, keepdims=True)\n",
    "            Y_TNK = rng.multinomial(data, P_TNK)\n",
    "\n",
    "            # Update latent factors\n",
    "            post_shp = shp + Y_TNK.sum(axis=1)\n",
    "            post_rte = rte + np.einsum('tn,kn->tk', mask, Z[s-1])\n",
    "            H[s] = rng.gamma(post_shp, 1. / post_rte)\n",
    "\n",
    "            post_shp = shp + Y_TNK.sum(axis=0).T\n",
    "            post_rte = rte + np.einsum('tn,kn->kt', mask, H[s])\n",
    "            Z[s] = rng.gamma(post_shp, 1. / post_rte)\n",
    "        \n",
    "        posterior_samples = {'Z': torch.from_numpy(Z[warmup_steps:]), \n",
    "                             'H': torch.from_numpy(H[warmup_steps:])}\n",
    "        return posterior_samples\n",
    "\n",
    "def run_NUTS_with_mask(model, data, mask, warmup_steps, num_samples):\n",
    "    \"\"\"Runs NUTS with given model, data, mask and returns posterior samples.\"\"\"\n",
    "    \n",
    "    pyro.clear_param_store() # do we need this?\n",
    "\n",
    "    # Define MCMC kernel\n",
    "    kernel = mcmc.NUTS(model)\n",
    "    mcmc_run = mcmc.MCMC(kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
    "\n",
    "    # Run MCMC process on our data with given latent dimension and mask\n",
    "    mcmc_run.run(data, mask)\n",
    "\n",
    "    # Extract samples\n",
    "    posterior_samples = mcmc_run.get_samples()\n",
    "    \n",
    "    return posterior_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_predictive_check(data, mask, model, posterior_samples):\n",
    "    model_name = model.__class__.__name__\n",
    "    posterior_predictive_samples = model.likelihood(**posterior_samples).sample()\n",
    "\n",
    "    log_prob_fake = model.likelihood(**posterior_samples).log_prob(posterior_predictive_samples)\n",
    "    log_prob_true = model.likelihood(**posterior_samples).log_prob(data)\n",
    "\n",
    "    d_fake = -log_prob_fake[:, ~mask].sum(axis=-1)\n",
    "    d_true = -log_prob_true[:, ~mask].sum(axis=-1)\n",
    "\n",
    "    ppop = (d_fake > d_true).float().mean()\n",
    "    return ppop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(data, mask_type=\"Random\"):\n",
    "    assert mask_type in [\"Random\", \"Speckled\", \"End\", \"None\"]\n",
    "\n",
    "    n_time, n_unit = data.shape\n",
    "    mask = torch.ones(n_time, n_unit)\n",
    "\n",
    "    if mask_type == \"Random\":\n",
    "        mask_rows = torch.randperm(n_time)[:20]\n",
    "        mask_cols = torch.randperm(n_unit)[:5]\n",
    "        for t in mask_rows:\n",
    "            for i in mask_cols:\n",
    "                mask[t, i] = 0\n",
    "    \n",
    "    elif mask_type == \"Speckled\":\n",
    "        # hold out 1% of the data\n",
    "        mask = torch.bernoulli(mask * 0.99)\n",
    "        \n",
    "    elif mask_type == \"End\":\n",
    "        mask[-30:, -3:] = 0\n",
    "    \n",
    "    mask = mask.bool()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Running GAP for Atlanta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 3000/3000 [15:11:07, 18.22s/it, step size=1.32e-03, acc. prob=0.881]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population predictive check p-value: 0.000\n",
      "\n",
      "Running PPCA for Atlanta...\n",
      "\n",
      "----------------------------------------\n",
      "Running GAP for LA Rams...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 3000/3000 [24:14:11, 29.08s/it, step size=5.23e-04, acc. prob=0.930]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population predictive check p-value: 0.000\n",
      "\n",
      "Running PPCA for LA Rams...\n",
      "\n",
      "----------------------------------------\n",
      "Running GAP for Washington...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 3000/3000 [23:37,  2.12it/s, step size=4.16e-02, acc. prob=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population predictive check p-value: 0.094\n",
      "\n",
      "Running PPCA for Washington...\n",
      "\n",
      "----------------------------------------\n",
      "Running GAP for New Orleans...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 3000/3000 [2:16:40,  2.73s/it, step size=9.58e-04, acc. prob=0.900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population predictive check p-value: 0.000\n",
      "\n",
      "Running PPCA for New Orleans...\n",
      "\n",
      "----------------------------------------\n",
      "Running GAP for Kansas City...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 3000/3000 [13:27,  3.72it/s, step size=5.92e-02, acc. prob=0.838]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected value argument (Tensor of shape (187, 86)) to be within the support (IntegerGreaterThan(lower_bound=0)) of the distribution Poisson(rate: torch.Size([2000, 187, 86])), but found invalid values:\ntensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n         1.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n         1.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n         1.0000e+00],\n        ...,\n        [3.4000e+01, 4.7000e+01, 1.2800e+02,  ..., 1.4050e+02, 3.1000e+01,\n         8.0100e+03],\n        [3.4000e+01, 4.7000e+01, 1.5900e+02,  ..., 1.4150e+02, 3.1000e+01,\n         8.0405e+03],\n        [3.4000e+01, 4.8000e+01, 1.5900e+02,  ..., 1.4350e+02, 3.2000e+01,\n         8.1515e+03]], dtype=torch.float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t4/2vpdk4_n5t1822kh1t723dn00000gn/T/ipykernel_6604/1789861752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Run population predictive check (this will re-generate posterior predictive samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         ppop = population_predictive_check(data=train_data, \n\u001b[0m\u001b[1;32m     60\u001b[0m                                            \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t4/2vpdk4_n5t1822kh1t723dn00000gn/T/ipykernel_6604/1083624711.py\u001b[0m in \u001b[0;36mpopulation_predictive_check\u001b[0;34m(data, mask, model, posterior_samples)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlog_prob_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mposterior_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposterior_predictive_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlog_prob_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mposterior_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0md_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlog_prob_fake\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyro/distributions/torch.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/distributions/poisson.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0;34m\"Expected value argument \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (187, 86)) to be within the support (IntegerGreaterThan(lower_bound=0)) of the distribution Poisson(rate: torch.Size([2000, 187, 86])), but found invalid values:\ntensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n         1.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n         1.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n         1.0000e+00],\n        ...,\n        [3.4000e+01, 4.7000e+01, 1.2800e+02,  ..., 1.4050e+02, 3.1000e+01,\n         8.0100e+03],\n        [3.4000e+01, 4.7000e+01, 1.5900e+02,  ..., 1.4150e+02, 3.1000e+01,\n         8.0405e+03],\n        [3.4000e+01, 4.8000e+01, 1.5900e+02,  ..., 1.4350e+02, 3.2000e+01,\n         8.1515e+03]], dtype=torch.float64)"
     ]
    }
   ],
   "source": [
    "# Define MCMC parameters\n",
    "warmup_steps = 1000\n",
    "num_samples = 2000\n",
    "\n",
    "# Data directory (must already exist)\n",
    "dat_path = Path('dat')\n",
    "assert dat_path.exists()\n",
    "\n",
    "# Traverse through directory and find training data tensors\n",
    "for train_data_file in dat_path.walkfiles('train_data.pt'):\n",
    "    # Get team name from directory\n",
    "    team = train_data_file.parent.splitpath()[-1]\n",
    "    if team in ['Jacksonville', 'Las Vegas', 'Tennessee', 'Detroit']:\n",
    "        continue\n",
    "\n",
    "    print('\\n----------------------------------------')\n",
    "\n",
    "    # Load training data tensor\n",
    "    train_data = torch.load(train_data_file)\n",
    "\n",
    "    # Get latent dimension\n",
    "    latent_dim = latent_dim_dict[team]\n",
    "\n",
    "    # Define model classes\n",
    "    models = [GAP(latent_dim=latent_dim, shp=1.0, rte=1.0),\n",
    "              PPCA(latent_dim=latent_dim, variance_support=(0, 10))]\n",
    "\n",
    "    # Create a results directory\n",
    "    out_path = Path('out').joinpath(team)\n",
    "    out_path.makedirs_p()\n",
    "\n",
    "    # Create and save mask (use .pt extension for tensor)\n",
    "    mask = create_mask(train_data, mask_type=\"Speckled\")\n",
    "    torch.save(mask, out_path.joinpath('mask.pt'))\n",
    "\n",
    "    # Run MCMC for each model\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        print(f\"Running {model_name} for {team}...\")\n",
    "\n",
    "        if model_name == 'PPCA':\n",
    "            continue\n",
    "\n",
    "        posterior_samples = run_NUTS_with_mask(model=model.model, \n",
    "                                               data=train_data,\n",
    "                                               mask=mask,\n",
    "                                               warmup_steps=warmup_steps,\n",
    "                                               num_samples=num_samples)\n",
    "        \n",
    "        # Save posterior samples (use .pth extension for dict of tensors)\n",
    "        torch.save(posterior_samples, out_path.joinpath(f'{model_name}_posterior_samples.pth'))\n",
    "\n",
    "        # Generate and save posterior predictive samples\n",
    "        posterior_predictive_samples = model.likelihood(**posterior_samples).sample()\n",
    "        assert posterior_predictive_samples.shape == (num_samples,) + train_data.shape\n",
    "        torch.save(posterior_predictive_samples, out_path.joinpath(f'{model_name}_posterior_predictive_samples.pt'))\n",
    "\n",
    "        # Run population predictive check (this will re-generate posterior predictive samples)\n",
    "        ppop = population_predictive_check(data=train_data, \n",
    "                                           mask=mask, \n",
    "                                           model=model,\n",
    "                                           posterior_samples=posterior_samples)\n",
    "        print(f\"Population predictive check p-value: {ppop:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(latent_dim_dict.keys()) * 2 * 5 * 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arizona': 5,\n",
       " 'Atlanta': 7,\n",
       " 'Baltimore': 15,\n",
       " 'Buffalo': 25,\n",
       " 'Carolina': 5,\n",
       " 'Chicago': 15,\n",
       " 'Cincinnati': 13,\n",
       " 'Cleveland': 1,\n",
       " 'Dallas': 37,\n",
       " 'Denver': 5,\n",
       " 'Detroit': 12,\n",
       " 'Green Bay': 52,\n",
       " 'Houston': 12,\n",
       " 'Indianapolis': 10,\n",
       " 'Jacksonville': 12,\n",
       " 'Kansas City': 11,\n",
       " 'Las Vegas': 12,\n",
       " 'LA Chargers': 5,\n",
       " 'LA Rams': 8,\n",
       " 'Miami': 8,\n",
       " 'Minnesota': 5,\n",
       " 'New England': 9,\n",
       " 'New Orleans': 18,\n",
       " 'NY Giants': 7,\n",
       " 'NY Jets': 7,\n",
       " 'Philadelphia': 10,\n",
       " 'Pittsburgh': 25,\n",
       " 'San Francisco': 10,\n",
       " 'Seattle': 14,\n",
       " 'Tampa Bay': 15,\n",
       " 'Tennessee': 13,\n",
       " 'Washington': 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "torch.Size([237, 142])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([231, 54])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([190, 17])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([187, 64])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([187, 86])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([205, 33])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([240, 101])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([198, 84])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([231, 54])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([213, 212])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([194, 16])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([206, 91])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([194, 212])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([182, 52])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([191, 84])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([195, 14])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([235, 17])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([199, 72])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([205, 42])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([217, 49])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([222, 52])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([198, 49])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([194, 16])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([226, 54])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([192, 72])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([211, 52])\n",
      "\n",
      "----------------------------------------\n",
      "torch.Size([186, 58])\n"
     ]
    }
   ],
   "source": [
    "# Data directory (must already exist)\n",
    "dat_path = Path('dat')\n",
    "assert dat_path.exists()\n",
    "\n",
    "# Traverse through directory and find training data tensors\n",
    "for train_data_file in dat_path.walkfiles('train_data.pt'):\n",
    "    # Get team name from directory\n",
    "    team = train_data_file.parent.splitpath()[-1]\n",
    "    if team in ['Jacksonville', 'Las Vegas', 'Tennessee', 'Detroit']:\n",
    "        continue\n",
    "\n",
    "    print('\\n----------------------------------------')\n",
    "\n",
    "    # Load training data tensor\n",
    "    train_data = torch.load(train_data_file)\n",
    "\n",
    "    print(train_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # Get latent dimension\n",
    "    # latent_dim = latent_dim_dict[team]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "237 / 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
